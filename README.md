# ğŸ§  Neural Network from Scratch (NumPy Implementation)

A simple **two-layer Neural Network** (built completely from scratch using NumPy) trained on the **MNIST handwritten digits dataset** (0â€“9).  
This project demonstrates how a neural network works **without using deep learning frameworks** like TensorFlow or PyTorch â€” only `NumPy`.

---

## ğŸš€ Project Overview

This project implements:
- A **feedforward neural network** with one hidden layer
- **ReLU** activation for the hidden layer
- **Softmax** activation for the output layer
- **Mini-batch gradient descent** for optimization
- **Cross-entropy loss** function for training
- **Multiple optimizers:**
    - SGD
    - Momentum
    - NAG (Nesterov Accelerated Gradient)
    - Adagrad
    - RMSProp
    - Adam
- **Accuracy evaluation** on test data

---

## ğŸ§© Architecture
```
Input (784 nodes: 28x28 image)
â†“
Hidden Layer (128 neurons, ReLU)
â†“
Output Layer (10 neurons, Softmax)
```
---
## ğŸ§° Project Structure
```
NN_from_scratch/
â”‚
â”œâ”€â”€ main.py          # Main training and evaluation script
â”œâ”€â”€ utils.py         # Utility functions (ReLU, Softmax, loss, accuracy)
â”œâ”€â”€ optimizer.py     # Implementation of various optimizers
â”œâ”€â”€ data/            # Folder containing MNIST dataset (mnist.npz)
â”œâ”€â”€ README.md        # Project documentation (youâ€™re reading this)
â”œâ”€â”€ requirements.txt # Dependencies (NumPy, scikit-learn, matplotlib)
â””â”€â”€ loss_histories.npy # Saved loss curves for all optimizers
```
---
## ğŸ“Š Results
- Trained the network with 6 different optimizers.
- Each optimizer has its own training loss curve.

![alt text](image/image.png)
---
![alt text](image/plot.png)
---

| Optimizer | Test Accuracy |
| --------- | ------------- |
| SGD       | 90.39%        |
| Momentum  | 96.51%        |
| NAG       | 96.69%        |
| Adagrad   | 96.84%        |
| RMSProp   | 97.53%        |
| Adam      | 97.23%        |

---

## ğŸ“˜ Key Learnings

- Understanding how forward and backward propagation work mathematically
- Implementing gradient descent and advanced optimizers manually
- Working with one-hot encoding for categorical targets
- Handling matrix operations efficiently in NumPy
- Building an end-to-end training loop from scratch
- Comparing optimizer performance on MNIST dataset

**â­ If you found this helpful, give the repo a star on GitHub!**